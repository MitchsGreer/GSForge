"""
``Analytics`` are intended to more closely rank or compare a GEM subset, rather than the entire GEM.
These functions are intended for analyzing and comparing subsets generated by the functions found in ``prospectors``.


Methods and notation from [method_compare]_ used.

: :math:`LS` : Learning Sample, :math:`n` instances of input-output values.
: :math:`n` : Number of input-output value pairs in :math:`LS`.
: :math:`m` : Number of input variables (features or genes) in :math:`LS`.
: :math `X_i` : Input array of :math:`LS`. Ranges from :math:`i=1, ..., m`.
: :math:`LS` : An algorithm that outputs some relevance score, :math:`s_i`, for each input variable :math `X_i`.


.. [method_compare] `A comparison of per sample global scaling and per gene normalization methods for differential expression analysis of RNA-seq data <https://doi.org/10.1371/journal.pone.0176185>`_

"""

# import warnings

import param
import xarray as xr
import numpy as np
from sklearn.base import clone

from ..models import OperationInterface
from ..utils._operations import shuffle_along_axis

__all__ = [
    "rank_genes_by_model",
    "nFDR",
    "mProbes",
]


# TODO: Add linter ignore for class capitaliation.
class rank_genes_by_model(OperationInterface):
    """
    Given some machine learning model, this operation runs n_iterations
    and returns a summary dataset of the ranking results.
    """
    # TODO: Note that this uses the OperationInterface.

    model = param.Parameter()
    n_iterations = param.Integer(default=1)

    def process(self):
        if self.n_iterations == 1:
            return self._rank_genes_by_model()
        else:
            rankings = [self._rank_genes_by_model() for _ in range(self.n_iterations)]
            ranking_ds = xr.concat(rankings, "feature_importance_iter")
            ranking_ds["feature_importance_iter"] = (["feature_importance_iter", ], np.arange(self.n_iterations))
            ranking_ds["feature_importance_mean"] = ([self.gem.gene_index_name],
                                                     ranking_ds.mean(dim="feature_importance_iter"))
            ranking_ds["feature_importance_std"] = ([self.gem.gene_index_name],
                                                    ranking_ds.std(dim="feature_importance_iter"))
            return ranking_ds.to_dataset()

    def _rank_genes_by_model(self):
        x_data = self.x_count_data
        y_data = self.y_annotation_data

        if isinstance(self.annotation_variables, list):
            y_data = y_data.to_dataframe().values

        model = self.model.fit(x_data, y_data)

        attrs = {'Ranking Model': str(model),
                 "count_variable": self.count_variable,
                 "annotation_variables": self.annotation_variables}

        data = xr.DataArray(data=model.feature_importances_,
                            dims=[self.gem.gene_index_name],
                            coords=[self.get_gene_index()],
                            name="feature_importances",
                            attrs=attrs)
        return data


# TODO: Consider moving to utils.
def _null_rank_distribution(real, shadow):
    """
    Returns the percent for which shadow values are ranked higher than a given real value.

    :param real: The 'real' scores.

    :param shadow: Scores from fake, or shadow, features.

    :return: An array of the same shape as ``real``, with the percent of times a shadow
      score had a higher value.
    """
    return np.sum(real[:, None] <= shadow[None, :], axis=-1) / shadow.shape[0]


class nFDR(OperationInterface):
    """
    nFDR (False Discovery Rate) [method_compare]_.

    nFDR trains two models and compares their ``feature_importances_`` attributes to estimate
    the false discovery rate.

    The FDR estimated is the percent of instances a shuffled output feature has a higher feature
    importance score than the same non-shuffled feature score.

    This is repeated up to ``n_iterations``.
    """

    model = param.Parameter()
    n_iterations = param.Integer(default=1)

    @staticmethod
    def nFDR(x_data, y_data, model):
        y_shadowed = np.random.permutation(y_data)

        real_model = clone(model).fit(x_data, y_data)
        shadow_model = clone(model).fit(x_data, y_shadowed)

        real_scores = real_model.feature_importances_
        shadow_scores = shadow_model.feature_importances_

        null_rank_dist = _null_rank_distribution(real_scores, shadow_scores)
        return null_rank_dist

    def _nFDR_to_xarray(self, nFRD_values):
        attrs = {'Ranking Model': str(self.model),
                 "count_variable": self.count_variable,
                 "annotation_variables": self.annotation_variables}
        return xr.DataArray(
            data=nFRD_values,
            coords=[self.get_gene_index()],
            dims=[self.gem.gene_index_name],
            attrs=attrs,
            name="nFDR")

    def process(self):

        x_data = self.x_count_data
        y_data = self.y_annotation_data

        if self.n_iterations == 1:
            nrd_values = self.nFDR(x_data, y_data, self.model)
            return self._nFDR_to_xarray(nrd_values)

        else:
            fdrs = [self.nFDR(x_data, y_data, self.model) for i in range(self.n_iterations)]
            fdrs = [self._nFDR_to_xarray(values) for values in fdrs]
            fdr_ds = xr.concat(fdrs, "nFDR_iter")
            fdr_ds.name = "nFDR"
            fdr_ds["nFDR_iter"] = (["nFDR_iter", ], np.arange(self.n_iterations))
            fdr_ds["nFDR_mean"] = ([self.gem.gene_index_name], fdr_ds.mean(dim="nFDR_iter"))
            fdr_ds["nFDR_std"] = ([self.gem.gene_index_name], fdr_ds.std(dim="nFDR_iter"))
            return fdr_ds.to_dataset()


class mProbes(OperationInterface):
    """
    mProbes [method_compare]_ works by randomly permuting the feature values in the supplied data.
    e.g. count values are shuffled within each samples feature (gene) array.

    It then ranks the real and shadowed features (for ``n_iterations``) with the supplied ``model``
    via a call to ``model.fit()``. It then examines ``model.feature_importances_`` for the feature
    importance values, and then calculates the null rank distribution.

    This is repeated upto ``n_iterations``.
    """

    model = param.Parameter()
    n_iterations = param.Integer(default=1)

    def process(self):
        if self.n_iterations == 1:
            return self._calculate_null_rank_distribution()
        else:
            rankings = [self._calculate_null_rank_distribution() for i in range(self.n_iterations)]
            ranking_ds = xr.concat(rankings, "nrd_iter")
            ranking_ds.name = "null rank distribution"
            ranking_ds["nrd_iter"] = (["nrd_iter", ], np.arange(self.n_iterations))
            ranking_ds["nrd_mean"] = ([self.gem.gene_index_name], ranking_ds.mean(dim="nrd_iter"))
            ranking_ds["nrd_std"] = ([self.gem.gene_index_name], ranking_ds.std(dim="nrd_iter"))
            return ranking_ds.to_dataset()

    def _calculate_null_rank_distribution(self):
        shadowed_array = shuffle_along_axis(self.x_count_data.values, 1)
        shadowed_dataset = np.hstack((self.x_count_data.values, shadowed_array))

        x_values = shadowed_dataset[self.count_variable].values
        y_values = self.y_annotation_data

        model = self.model.fit(x_values, y_values)
        ranks = model.feature_importances_
        real, shadow = ranks.reshape((2, self.get_gene_index().shape[0]))
        null_rank_dist = _null_rank_distribution(real, shadow)

        attrs = {'Ranking Model': str(model),
                 "count_variable": self.count_variable,
                 "annotation_variables": self.annotation_variables}

        return xr.DataArray(data=null_rank_dist, coords=[self.get_gene_index()],
                            dims=[self.gem.gene_index_name], attrs=attrs,
                            name="null_rank_distribution")

#
# class calculate_family_wise_error_rates(OperationInterface):
#     # TODO: Cite and update docstring.
#
#     """
#     Estimates the false-discovery rate (type I errors).
#     """
#     model = param.Parameter()
#     n_iterations = param.Integer(default=1)
#
#     def process(self):
#         if self.n_iterations == 1:
#             return self._calculate_family_wise_error_rates()
#         else:
#             rankings = [self._calculate_family_wise_error_rates() for i in range(self.n_iterations)]
#             ranking_ds = xr.concat(rankings, "fwe_iter")
#             ranking_ds.name = "Family-wise error rate"
#             ranking_ds["fwe_iter"] = (["fwe_iter", ], np.arange(self.n_iterations))
#             ranking_ds["fwe_mean"] = ([self.gem.gene_index_name], ranking_ds.mean(dim="fwe_iter"))
#             ranking_ds["fwe_std"] = ([self.gem.gene_index_name], ranking_ds.std(dim="fwe_iter"))
#             return ranking_ds.to_dataset()
#
#     def _calculate_family_wise_error_rates(self):
#         random_values = np.random.randn(*self.x_count_data.shape)
#         shadowed_values = np.hstack([self.x_count_data, random_values])
#
#         y_values = self.y_annotation_data
#
#         if isinstance(self.annotation_variables, list):
#             y_values = y_values.to_dataframe().values
#
#         model = self.model.fit(shadowed_values, y_values)
#         ranks = model.feature_importances_
#         real, shadow = ranks.reshape((2, self.x_count_data.shape[1]))
#
#         null_rank_dist = _null_rank_distribution(real, shadow)
#
#         attrs = {'Ranking Model': str(model),
#                  "count_variable": self.count_variable,
#                  "annotation_variables": self.annotation_variables}
#
#         return xr.DataArray(data=null_rank_dist, coords=[self.get_gene_index()],
#                             dims=[self.gem.gene_index_name], attrs=attrs,
#                             name="family_wise_error_rate")
